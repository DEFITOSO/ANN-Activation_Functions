# Artificial Neural Networks Activation Functions

This repository contains Python code examples for various activation functions commonly used in Artificial Neural Networks (ANNs). Each activation function is implemented in a separate folder, and the code provides a clear demonstration of how to use and visualize the activation functions.

## Activation Functions Included

1. **Sigmoid Function**
   - Folder: `Neural-Network_Sigmoid`
   - Description: Implementation and demonstration of the sigmoid activation function.

2. **Hyperbolic Tangent (tanh) Function**
   - Folder: `Neural-Network_Tanh`
   - Description: Implementation and demonstration of the hyperbolic tangent activation function.

3. **Rectified Linear Unit (ReLU) Function**
   - Folder: `Neural-Network_ReLU`
   - Description: Implementation and demonstration of the Rectified Linear Unit (ReLU) activation function.

4. **Leaky Rectified Linear Unit (Leaky ReLU) Function**
   - Folder: `Neural-Network_ReLUleakely`
   - Description: Implementation and demonstration of the Leaky Rectified Linear Unit (Leaky ReLU) activation function.

5. **Exponential Linear Unit (ELU) Function**
   - Folder: `Neural-Network_ELU`
   - Description: Implementation and demonstration of the Exponential Linear Unit (ELU) activation function.

## How to Use

Each activation function has its own dedicated folder. Inside each folder, you'll find a Python script demonstrating the implementation and usage of the respective activation function. Additionally, there might be Jupyter notebooks or additional resources.

Feel free to explore the folders based on the activation functions you are interested in. The code is well-documented, and you can easily adapt it for your own projects.

## Getting Started

To get started, clone this repository to your local machine:

```bash
git clone https://github.com/your-username/your-repository.git
